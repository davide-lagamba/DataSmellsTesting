{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qxc74q81LkiM",
    "outputId": "7ee91074-523c-4b40-f9a2-d16780df97e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sdv\n",
      "  Downloading sdv-1.17.2-py3-none-any.whl (154 kB)\n",
      "Collecting sdmetrics>=0.17.0\n",
      "  Downloading sdmetrics-0.17.0-py3-none-any.whl (174 kB)\n",
      "Collecting boto3<2.0.0,>=1.28\n",
      "  Downloading boto3-1.35.74-py3-none-any.whl (139 kB)\n",
      "Collecting graphviz>=0.13.2\n",
      "  Using cached graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "Collecting copulas>=0.12.0\n",
      "  Downloading copulas-0.12.0-py3-none-any.whl (52 kB)\n",
      "Requirement already satisfied: pandas>=1.4.0 in c:\\users\\utente\\desktop\\datasmellstesting\\.v\\lib\\site-packages (from sdv) (2.0.3)\n",
      "Collecting botocore<2.0.0,>=1.31\n",
      "  Downloading botocore-1.35.74-py3-none-any.whl (13.2 MB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\utente\\desktop\\datasmellstesting\\.v\\lib\\site-packages (from sdv) (1.24.4)\n",
      "Collecting rdt>=1.13.1\n",
      "  Downloading rdt-1.13.1-py3-none-any.whl (66 kB)\n",
      "Requirement already satisfied: tqdm>=4.29 in c:\\users\\utente\\desktop\\datasmellstesting\\.v\\lib\\site-packages (from sdv) (4.66.5)\n",
      "Collecting cloudpickle>=2.1.0\n",
      "  Downloading cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: platformdirs>=4.0 in c:\\users\\utente\\desktop\\datasmellstesting\\.v\\lib\\site-packages (from sdv) (4.3.2)\n",
      "Collecting ctgan>=0.10.2\n",
      "  Using cached ctgan-0.10.2-py3-none-any.whl (23 kB)\n",
      "Collecting pyyaml>=6.0.1\n",
      "  Downloading PyYAML-6.0.2-cp38-cp38-win_amd64.whl (162 kB)\n",
      "Collecting deepecho>=0.6.1\n",
      "  Using cached deepecho-0.6.1-py3-none-any.whl (27 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0\n",
      "  Downloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting urllib3<1.27,>=1.25.4\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\utente\\desktop\\datasmellstesting\\.v\\lib\\site-packages (from botocore<2.0.0,>=1.31->sdv) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.7.3 in c:\\users\\utente\\desktop\\datasmellstesting\\.v\\lib\\site-packages (from copulas>=0.12.0->sdv) (1.10.1)\n",
      "Requirement already satisfied: plotly>=5.10.0 in c:\\users\\utente\\desktop\\datasmellstesting\\.v\\lib\\site-packages (from copulas>=0.12.0->sdv) (5.24.0)\n",
      "Collecting torch>=1.9.0\n",
      "  Downloading torch-2.4.1-cp38-cp38-win_amd64.whl (199.4 MB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\utente\\desktop\\datasmellstesting\\.v\\lib\\site-packages (from pandas>=1.4.0->sdv) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\utente\\desktop\\datasmellstesting\\.v\\lib\\site-packages (from pandas>=1.4.0->sdv) (2024.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\utente\\desktop\\datasmellstesting\\.v\\lib\\site-packages (from plotly>=5.10.0->copulas>=0.12.0->sdv) (24.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\utente\\desktop\\datasmellstesting\\.v\\lib\\site-packages (from plotly>=5.10.0->copulas>=0.12.0->sdv) (9.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\utente\\desktop\\datasmellstesting\\.v\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.31->sdv) (1.16.0)\n",
      "Collecting Faker>=17\n",
      "  Downloading Faker-33.1.0-py3-none-any.whl (1.9 MB)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\utente\\desktop\\datasmellstesting\\.v\\lib\\site-packages (from rdt>=1.13.1->sdv) (1.3.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\utente\\desktop\\datasmellstesting\\.v\\lib\\site-packages (from Faker>=17->rdt>=1.13.1->sdv) (4.12.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\utente\\desktop\\datasmellstesting\\.v\\lib\\site-packages (from scikit-learn>=1.0.2->rdt>=1.13.1->sdv) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\utente\\desktop\\datasmellstesting\\.v\\lib\\site-packages (from scikit-learn>=1.0.2->rdt>=1.13.1->sdv) (1.4.2)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Collecting jinja2\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Requirement already satisfied: colorama in c:\\users\\utente\\desktop\\datasmellstesting\\.v\\lib\\site-packages (from tqdm>=4.29->sdv) (0.4.6)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.5-cp38-cp38-win_amd64.whl (17 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: urllib3, mpmath, MarkupSafe, jmespath, sympy, networkx, jinja2, fsspec, filelock, Faker, botocore, torch, s3transfer, rdt, copulas, sdmetrics, pyyaml, graphviz, deepecho, ctgan, cloudpickle, boto3, sdv\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "Successfully installed Faker-33.1.0 MarkupSafe-2.1.5 boto3-1.35.74 botocore-1.35.74 cloudpickle-3.1.0 copulas-0.12.0 ctgan-0.10.2 deepecho-0.6.1 filelock-3.16.1 fsspec-2024.10.0 graphviz-0.20.3 jinja2-3.1.4 jmespath-1.0.1 mpmath-1.3.0 networkx-3.1 pyyaml-6.0.2 rdt-1.13.1 s3transfer-0.10.4 sdmetrics-0.17.0 sdv-1.17.2 sympy-1.13.3 torch-2.4.1 urllib3-1.26.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\utente\\desktop\\datasmellstesting\\.v\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install sdv\n",
    "import numpy as np\n",
    "#tutorial https://docs.sdv.dev/sdv\n",
    "import sdv\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Irqz7cwiMAzE"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df_l_1_path = \"./cleaned_datasets/df_l_1.csv\"  #kdd-census\n",
    "    df_l_2_path = \"./cleaned_datasets/df_l_2.csv\" #diabetic_data\n",
    "    df_m_1_path = \"./cleaned_datasets/df_m_1.csv\" #Firefighter_Promotion_Exam_Scores\n",
    "    df_l_3_path = \"./cleaned_datasets/df_l_3.csv\" #nursery\n",
    "    df_m_2_path = \"./cleaned_datasets/df_m_2.csv\" #tae\n",
    "    df_l_1 = pd.read_csv(df_l_1_path, index_col=False)\n",
    "    df_l_2 = pd.read_csv(df_l_2_path, index_col=False)\n",
    "    df_m_1 = pd.read_csv(df_m_1_path, index_col=False)\n",
    "    df_l_3 = pd.read_csv(df_l_3_path, index_col=False)\n",
    "    df_m_2 = pd.read_csv(df_m_2_path, index_col=False)\n",
    "\n",
    "    if not Path(\"metadata\").exists():\n",
    "      os.mkdir(\"metadata\")\n",
    "\n",
    "    if not Path(\"synthesizers\").exists():\n",
    "      os.mkdir(\"synthesizers\")\n",
    "\n",
    "    if not Path(\"outputSDV\").exists():\n",
    "      os.mkdir(\"outputSDV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SEatpVdmMFMU"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    from sdv.metadata import Metadata\n",
    "\n",
    "    metadata_l_1 = Metadata.detect_from_dataframe(\n",
    "        data=df_l_1,\n",
    "        table_name='kdd-census')\n",
    "\n",
    "    metadata_l_1.validate()\n",
    "    filepath_l_1 = \"metadata/meta_df_l_1.json\"\n",
    "\n",
    "\n",
    "    metadata_l_2 = Metadata.detect_from_dataframe(\n",
    "        data=df_l_2,\n",
    "        table_name='diabetic_data')\n",
    "\n",
    "    metadata_l_2.validate()\n",
    "    filepath_l_2 = \"metadata/meta_df_l_2.json\"\n",
    "\n",
    "\n",
    "    metadata_l_3 = Metadata.detect_from_dataframe(\n",
    "        data=df_l_3,\n",
    "        table_name='nursery')\n",
    "\n",
    "    metadata_l_3.validate()\n",
    "    filepath_l_3 = \"metadata/meta_df_l_3.json\"\n",
    "\n",
    "\n",
    "    metadata_m_1 = Metadata.detect_from_dataframe(\n",
    "        data=df_m_1,\n",
    "        table_name='Firefighter_Promotion_Exam_Scores')\n",
    "\n",
    "    metadata_m_1.validate()\n",
    "    filepath_m_1 = \"metadata/meta_df_m_1.json\"\n",
    "\n",
    "\n",
    "    metadata_m_2 = Metadata.detect_from_dataframe(\n",
    "        data=df_m_2,\n",
    "        table_name='tae')\n",
    "\n",
    "    metadata_m_2.validate()\n",
    "    filepath_m_2 = \"metadata/meta_df_m_2.json\"\n",
    "\n",
    "\n",
    "\n",
    "    if not Path(filepath_l_1).exists():\n",
    "      metadata_l_1.save_to_json(filepath=filepath_l_1)\n",
    "      \n",
    "    if not Path(filepath_l_2).exists():\n",
    "      metadata_l_2.save_to_json(filepath=filepath_l_2)\n",
    "\n",
    "    if not Path(filepath_l_3).exists():\n",
    "      metadata_l_3.save_to_json(filepath=filepath_l_3)\n",
    "\n",
    "    if not Path(filepath_m_1).exists():\n",
    "      metadata_m_1.save_to_json(filepath=filepath_m_1)  \n",
    "\n",
    "    if not Path(filepath_m_2).exists():\n",
    "      metadata_m_2.save_to_json(filepath=filepath_m_2)\n",
    "\n",
    "    from sdv.single_table import CTGANSynthesizer\n",
    "\n",
    "    filepath_synthesizer_l_1 = \"synthesizers/synthesizer_df_l_1.pkl\"\n",
    "    filepath_metadata_synthesizer_l_1 = \"synthesizers/meta_synthesizer_df_l_1.json\"\n",
    "\n",
    "    filepath_synthesizer_l_2 = \"synthesizers/synthesizer_df_l_2.pkl\"\n",
    "    filepath_metadata_synthesizer_l_2 = \"synthesizers/meta_synthesizer_df_l_2.json\"\n",
    "\n",
    "    filepath_synthesizer_l_3 = \"synthesizers/synthesizer_df_l_3.pkl\"\n",
    "    filepath_metadata_synthesizer_l_3 = \"synthesizers/meta_synthesizer_df_l_3.json\"\n",
    "\n",
    "    filepath_synthesizer_m_1 = \"synthesizers/synthesizer_df_m_1.pkl\"\n",
    "    filepath_metadata_synthesizer_m_1 = \"synthesizers/meta_synthesizer_df_m_1.json\"\n",
    "\n",
    "    filepath_synthesizer_m_2 = \"synthesizers/synthesizer_df_m_2.pkl\"\n",
    "    filepath_metadata_synthesizer_m_2 = \"synthesizers/meta_synthesizer_df_m_2.json\"\n",
    "\n",
    "    def create_synthesizer(df, metadata_df, filepath_synthesizer, filepath_metadata_synthesizer):\n",
    "      if not Path(filepath_synthesizer).exists():\n",
    "        synthesizer = CTGANSynthesizer(metadata_df)\n",
    "        synthesizer.fit(df)\n",
    "        synthesizer.save(filepath=filepath_synthesizer)\n",
    "        synthesizer.get_metadata().save_to_json(filepath=filepath_metadata_synthesizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RbIbmtG-MIIN"
   },
   "outputs": [],
   "source": [
    "    create_synthesizer(df_l_1, metadata_l_1, filepath_synthesizer_l_1, filepath_metadata_synthesizer_l_1)\n",
    "\n",
    "    create_synthesizer(df_l_2, metadata_l_2, filepath_synthesizer_l_2, filepath_metadata_synthesizer_l_2)\n",
    "\n",
    "    create_synthesizer(df_l_3, metadata_l_3, filepath_synthesizer_l_3, filepath_metadata_synthesizer_l_3)\n",
    "\n",
    "    create_synthesizer(df_m_1, metadata_m_1, filepath_synthesizer_m_1, filepath_metadata_synthesizer_m_1)\n",
    "\n",
    "    create_synthesizer(df_m_2, metadata_m_2, filepath_synthesizer_m_2, filepath_metadata_synthesizer_m_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "id": "4VWCtcycMLm1",
    "outputId": "48d0e1dd-0f97-45f1-8314-acdb4a9c82d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sdv/_utils.py:290: SDV Version Warning: You are currently on SDV version 1.17.2 but this synthesizer was created on version 1.17.1. The latest bug fixes and features may not be available for this synthesizer. To see these enhancements, create and train a new synthesizer on this version.\n",
      "  warnings.warn(message, SDVVersionWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\nsynthesizer_l_3 = CTGANSynthesizer.load(filepath=filepath_synthesizer_l_3)\\n\\nsynthetic_data_l_3 = synthesizer_l_3.sample(num_rows=math.ceil((len(df_l_3) * 100) / 90) - len(df_l_3)+1) # +1 per raggiungere precisamente la percentuale di dati iniettati richiesta a seguito dell\\'unione con il dataset originale\\n\\nsynthetic_data_l_3.to_csv(\"outputSDV/df_l_3_SDV.csv\", index=False)\\n\\nsynthetic_data_l_3 = synthesizer_l_3.sample(num_rows=math.ceil((len(df_l_3) * 100) / 95) - len(df_l_3))\\nsynthetic_data_l_3.to_csv(\"outputSDV/df_l_3_95_SDV.csv\", index=False)\\n\\nsynthetic_data_l_3 = synthesizer_l_3.sample(num_rows=math.ceil((len(df_l_3) * 100) / 99) - len(df_l_3))\\nsynthetic_data_l_3.to_csv(\"outputSDV/df_l_3_99_SDV.csv\", index=False)\\n\\n\\nsynthesizer_m_1 = CTGANSynthesizer.load(filepath=filepath_synthesizer_m_1)\\n\\n#print(synthesizer_m_1.get_distributions())\\n\\nsynthetic_data_m_1 = synthesizer_m_1.sample(num_rows=math.ceil((len(df_m_1) * 100) / 90) - len(df_m_1))\\nsynthetic_data_m_1.to_csv(\"outputSDV/df_m_1_SDV.csv\", index=False)\\n\\nsynthetic_data_m_1 = synthesizer_m_1.sample(num_rows=math.ceil((len(df_m_1) * 100) / 95) - len(df_m_1))\\nsynthetic_data_m_1.to_csv(\"outputSDV/df_m_1_95_SDV.csv\", index=False)\\n\\nsynthetic_data_m_1 = synthesizer_m_1.sample(num_rows=math.ceil((len(df_m_1) * 100) / 99) - len(df_m_1))\\nsynthetic_data_m_1.to_csv(\"outputSDV/df_m_1_99_SDV.csv\", index=False)\\n\\nsynthetic_data_m_1 = synthesizer_m_1.sample(num_rows=1)\\nsynthetic_data_m_1.to_csv(\"outputSDV/df_m_1_FP_SDV.csv\", index=False)\\n\\nsynthesizer_m_2 = CTGANSynthesizer.load(filepath=filepath_synthesizer_m_2)\\n\\nsynthetic_data_m_2 = synthesizer_m_2.sample(num_rows=math.ceil((len(df_m_2) * 100) / 90) - len(df_m_2))\\nsynthetic_data_m_2.to_csv(\"outputSDV/df_m_2_SDV.csv\", index=False)\\n\\nsynthetic_data_m_2 = synthesizer_m_2.sample(num_rows=math.ceil((len(df_m_2) * 100) / 95) - len(df_m_2))\\nsynthetic_data_m_2.to_csv(\"outputSDV/df_m_2_95_SDV.csv\", index=False)\\n\\nsynthetic_data_m_2 = synthesizer_m_2.sample(num_rows=math.ceil((len(df_m_2) * 100) / 99) - len(df_m_2))\\nsynthetic_data_m_2.to_csv(\"outputSDV/df_m_2_99_SDV.csv\", index=False)\\n\\n#condition_m_2 = Condition(num_rows=math.ceil((len(df_m_2) * 100) / 99) - len(df_m_2), column_values={\"Course\": 36})\\n#synthesizer_m_2.sample_from_conditions(conditions=[condition_m_2], output_file_path=\"outputSDV/df_m_2_EV_SDV.csv\")\\n\\n#synthesizer_l_2 = CTGANSynthesizer.load(filepath=filepath_synthesizer_l_2)\\n\\n#synthetic_data_l_2 = synthesizer_l_2.sample(num_rows=math.ceil((len(df_l_2)*100)/90)-len(df_l_2))\\n#synthetic_data_l_2.to_csv(\"outputSDV/df_l_2_SDV.csv\", index = False)\\n\\n\\n#synthesizer_l_3 = CTGANSynthesizer.load(filepath=filepath_synthesizer_l_3)\\n\\n#synthetic_data_l_3 = synthesizer_l_3.sample(num_rows=math.ceil((len(df_l_3)*100)/90)-len(df_l_3))\\n#synthetic_data_l_3.to_csv(\"outputSDV/df_l_3_SDV.csv\", index = False)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    import math\n",
    "    from sdv.sampling import Condition\n",
    "    \n",
    "    synthesizer_l_1 = CTGANSynthesizer.load(filepath=filepath_synthesizer_l_1)\n",
    "\n",
    "    metadata = synthesizer_l_1.get_metadata().load_from_json(filepath_metadata_synthesizer_l_1)\n",
    "\n",
    "    synthetic_data_l_1 = synthesizer_l_1.sample(num_rows=math.ceil((len(df_l_1) * 100) / 90) - len(df_l_1))\n",
    "    synthetic_data_l_1.to_csv(\"outputSDV/df_l_1_SDV.csv\", index=False)\n",
    "\n",
    "    synthetic_data_l_1 = synthesizer_l_1.sample(num_rows=math.ceil((len(df_l_1) * 100) / 95) - len(df_l_1))\n",
    "    synthetic_data_l_1.to_csv(\"outputSDV/df_l_1_95_SDV.csv\", index=False)\n",
    "\n",
    "    synthetic_data_l_1 = synthesizer_l_1.sample(num_rows=math.ceil((len(df_l_1) * 100) / 99) - len(df_l_1))\n",
    "    synthetic_data_l_1.to_csv(\"outputSDV/df_l_1_99_SDV.csv\", index=False)\n",
    "\n",
    "    synthesizer_l_2 = CTGANSynthesizer.load(filepath=filepath_synthesizer_l_2)\n",
    "\n",
    "    synthetic_data_l_2 = synthesizer_l_2.sample(num_rows=math.ceil((len(df_l_2) * 100) / 90) - len(df_l_2))\n",
    "    synthetic_data_l_2.to_csv(\"outputSDV/df_l_2_SDV.csv\", index=False)\n",
    "\n",
    "    synthesizer_l_2.get_metadata().save_to_json(filepath=filepath_metadata_synthesizer_l_2)\n",
    "\n",
    "    synthetic_data_l_2 = synthesizer_l_2.sample(num_rows=math.ceil((len(df_l_2) * 100) / 95) - len(df_l_2))\n",
    "    synthetic_data_l_2.to_csv(\"outputSDV/df_l_2_95_SDV.csv\", index=False)\n",
    "\n",
    "    synthetic_data_l_2 = synthesizer_l_2.sample(num_rows=math.ceil((len(df_l_2) * 100) / 99) - len(df_l_2))\n",
    "    synthetic_data_l_2.to_csv(\"outputSDV/df_l_2_99_SDV.csv\", index=False)\n",
    "    \n",
    "\n",
    "    synthesizer_l_3 = CTGANSynthesizer.load(filepath=filepath_synthesizer_l_3)\n",
    "\n",
    "    synthetic_data_l_3 = synthesizer_l_3.sample(num_rows=math.ceil((len(df_l_3) * 100) / 90) - len(df_l_3)+1) # +1 per raggiungere precisamente la percentuale di dati iniettati richiesta a seguito dell'unione con il dataset originale\n",
    "\n",
    "    synthetic_data_l_3.to_csv(\"outputSDV/df_l_3_SDV.csv\", index=False)\n",
    "\n",
    "    synthetic_data_l_3 = synthesizer_l_3.sample(num_rows=math.ceil((len(df_l_3) * 100) / 95) - len(df_l_3))\n",
    "    synthetic_data_l_3.to_csv(\"outputSDV/df_l_3_95_SDV.csv\", index=False)\n",
    "\n",
    "    synthetic_data_l_3 = synthesizer_l_3.sample(num_rows=math.ceil((len(df_l_3) * 100) / 99) - len(df_l_3))\n",
    "    synthetic_data_l_3.to_csv(\"outputSDV/df_l_3_99_SDV.csv\", index=False)\n",
    "\n",
    "\n",
    "    synthesizer_m_1 = CTGANSynthesizer.load(filepath=filepath_synthesizer_m_1)\n",
    "\n",
    "    synthetic_data_m_1 = synthesizer_m_1.sample(num_rows=math.ceil((len(df_m_1) * 100) / 90) - len(df_m_1))\n",
    "    synthetic_data_m_1.to_csv(\"outputSDV/df_m_1_SDV.csv\", index=False)\n",
    "\n",
    "    synthetic_data_m_1 = synthesizer_m_1.sample(num_rows=math.ceil((len(df_m_1) * 100) / 95) - len(df_m_1))\n",
    "    synthetic_data_m_1.to_csv(\"outputSDV/df_m_1_95_SDV.csv\", index=False)\n",
    "\n",
    "    synthetic_data_m_1 = synthesizer_m_1.sample(num_rows=math.ceil((len(df_m_1) * 100) / 99) - len(df_m_1))\n",
    "    synthetic_data_m_1.to_csv(\"outputSDV/df_m_1_99_SDV.csv\", index=False)\n",
    "\n",
    "    synthetic_data_m_1 = synthesizer_m_1.sample(num_rows=1)\n",
    "    synthetic_data_m_1.to_csv(\"outputSDV/df_m_1_FP_SDV.csv\", index=False)\n",
    "\n",
    "    synthesizer_m_2 = CTGANSynthesizer.load(filepath=filepath_synthesizer_m_2)\n",
    "\n",
    "    synthetic_data_m_2 = synthesizer_m_2.sample(num_rows=math.ceil((len(df_m_2) * 100) / 90) - len(df_m_2))\n",
    "    synthetic_data_m_2.to_csv(\"outputSDV/df_m_2_SDV.csv\", index=False)\n",
    "\n",
    "    synthetic_data_m_2 = synthesizer_m_2.sample(num_rows=math.ceil((len(df_m_2) * 100) / 95) - len(df_m_2))\n",
    "    synthetic_data_m_2.to_csv(\"outputSDV/df_m_2_95_SDV.csv\", index=False)\n",
    "\n",
    "    synthetic_data_m_2 = synthesizer_m_2.sample(num_rows=math.ceil((len(df_m_2) * 100) / 99) - len(df_m_2))\n",
    "    synthetic_data_m_2.to_csv(\"outputSDV/df_m_2_99_SDV.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
